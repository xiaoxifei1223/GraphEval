"""æ¼”ç¤ºä½¿ç”¨REBELè¿›è¡ŒçŸ¥è¯†å›¾è°±æ„å»ºçš„å®Œæ•´æµç¨‹ã€‚

è¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•ï¼š
1. ä»Markdownæ–‡æ¡£ä¸­è¯»å–å†…å®¹
2. ä½¿ç”¨REBELæ¨¡å‹æå–çŸ¥è¯†å›¾è°±
3. å°†ç»“æœå¯è§†åŒ–å’ŒæŒä¹…åŒ–å­˜å‚¨
"""
import os
from grapheval.kg_construction.rebel_extractor import RebelExtractor, extract_kg_with_rebel
from grapheval.storage.graph_storage import persist_kg
from grapheval.kg_construction.llm_extractor import ExtractionResult


def read_markdown_file(file_path: str) -> str:
    """è¯»å–Markdownæ–‡ä»¶å†…å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


def extract_key_paragraphs(content: str, max_length: int = 1000) -> list[str]:
    """æå–æ–‡æ¡£ä¸­çš„å…³é”®æ®µè½ç”¨äºKGæ„å»º
    
    ç”±äºREBELæ¨¡å‹å¯¹å•æ¬¡è¾“å…¥æœ‰é•¿åº¦é™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦åˆ†æ®µå¤„ç†
    æ”¹è¿›ï¼šæŒ‰å¥å­åˆ†å‰²ä»¥æé«˜ä¸‰å…ƒç»„æå–çš„å‡†ç¡®æ€§
    """
    import re
    
    # æŒ‰è¡Œåˆ†å‰²
    lines = content.split('\n')
    
    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆå¥å­
    sentences = []
    
    for line in lines:
        line = line.strip()
        
        # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜
        if not line or line.startswith('#'):
            continue
            
        # è·³è¿‡ç®€å•åˆ—è¡¨é¡¹
        if line.startswith('- ') and len(line) < 30:
            continue
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­ï¼Œä¿ç•™å¥å°¾æ ‡ç‚¹
        sent_list = re.split(r'(?<=[.!?])\s+', line)
        
        for sent in sent_list:
            sent = sent.strip()
            # è¿‡æ»¤å¤ªçŸ­çš„å¥å­ï¼ˆå°‘äº20ä¸ªå­—ç¬¦ï¼‰
            if len(sent) > 20:
                sentences.append(sent)
    
    # é™åˆ¶å¥å­æ•°é‡ï¼Œé¿å…å¤„ç†æ—¶é—´è¿‡é•¿
    return sentences[:30]  # å–å‰30ä¸ªå¥å­


def main():
    print("=" * 80)
    print("çŸ¥è¯†å›¾è°±æ„å»ºæ¼”ç¤ºï¼šä»è½¯ä»¶éœ€æ±‚æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†")
    print("=" * 80)
    
    # 1. è¯»å–æ–‡æ¡£
    doc_path = "d:/MyCode/GraphEval/test_data/software_requirement_en.md"
    print(f"\nğŸ“– æ­¥éª¤1: è¯»å–æ–‡æ¡£\næ–‡ä»¶è·¯å¾„: {doc_path}")
    
    if not os.path.exists(doc_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {doc_path}")
        return
    
    content = read_markdown_file(doc_path)
    print(f"âœ… æ–‡æ¡£è¯»å–æˆåŠŸï¼Œæ€»å­—ç¬¦æ•°: {len(content)}")
    
    # 2. æå–å…³é”®å¥å­
    print(f"\nğŸ“ æ­¥éª¤2: æå–å…³é”®å¥å­")
    sentences = extract_key_paragraphs(content)
    print(f"âœ… æå–äº† {len(sentences)} ä¸ªå¥å­ç”¨äºåˆ†æ\n")
    
    # 3. åˆå§‹åŒ–REBELæå–å™¨
    print("ğŸ¤– æ­¥éª¤3: åˆå§‹åŒ–REBELæ¨¡å‹")
    extractor = RebelExtractor()
    print("âœ… REBELæ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    # 4. å¯¹æ¯ä¸ªå¥å­æå–çŸ¥è¯†å›¾è°±
    print("ğŸ” æ­¥éª¤4: æå–çŸ¥è¯†ä¸‰å…ƒç»„")
    print("-" * 80)
    
    all_triples = []
    all_entities = {}  # æ”¹ç”¨å­—å…¸ï¼šentity.text -> Entity
    
    for idx, sent in enumerate(sentences, 1):
        print(f"\nå¥å­ {idx}/{len(sentences)}:")
        print(f"å†…å®¹: {sent[:80]}..." if len(sent) > 80 else f"å†…å®¹: {sent}")
        
        triples = extractor.extract_relations(sent)
        
        if triples:
            print(f"âœ… æå–åˆ° {len(triples)} ä¸ªä¸‰å…ƒç»„:")
            for triple in triples:
                print(f"   â€¢ {triple.head.text} --[{triple.relation}]--> {triple.tail.text}")
                all_triples.append(triple)
                all_entities[triple.head.text] = triple.head
                all_entities[triple.tail.text] = triple.tail
        else:
            print("   âš ï¸ æœªæå–åˆ°ä¸‰å…ƒç»„")
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š æ€»ç»“:")
    print(f"   - æ€»å…±æå–äº† {len(all_triples)} ä¸ªå…³ç³»ä¸‰å…ƒç»„")
    print(f"   - è¯†åˆ«äº† {len(all_entities)} ä¸ªå”¯ä¸€å®ä½“")
    print("=" * 80)
    
    # 5. æŒä¹…åŒ–å­˜å‚¨
    if all_triples:
        print("\nğŸ’¾ æ­¥éª¤5: æŒä¹…åŒ–å­˜å‚¨çŸ¥è¯†å›¾è°±")
        
        # æ„å»ºExtractionResult
        kg_result = ExtractionResult(
            entities=list(all_entities.values()),
            triples=all_triples
        )
        
        # å­˜å‚¨åˆ°JSONå’ŒNetworkX
        output_json = "d:/MyCode/GraphEval/test_data/kg_output_en.json"
        summary = persist_kg(
            kg_result,
            json_path=output_json,
            build_networkx=True,
        )
        
        print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {summary['json_path']}")
        if summary['networkx_graph']:
            G = summary['networkx_graph']
            print(f"âœ… NetworkXå›¾æ„å»ºå®Œæˆ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
        
        # 6. å±•ç¤ºå…³é”®å®ä½“
        print("\nğŸ‘¥ æ­¥éª¤6: å…³é”®å®ä½“è¯†åˆ«")
        print("-" * 80)
        
        # ç»Ÿè®¡å®ä½“å‡ºç°é¢‘ç‡
        entity_freq = {}
        for triple in all_triples:
            head_text = triple.head.text
            tail_text = triple.tail.text
            entity_freq[head_text] = entity_freq.get(head_text, 0) + 1
            entity_freq[tail_text] = entity_freq.get(tail_text, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        top_entities = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        print("å‡ºç°é¢‘ç‡æœ€é«˜çš„10ä¸ªå®ä½“:")
        for entity, freq in top_entities:
            print(f"   â€¢ {entity}: {freq} æ¬¡")
    
    print("\n" + "=" * 80)
    print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()
